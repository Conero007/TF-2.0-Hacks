{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs with TF 2.0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/GANs_with_TF_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9vaOlDrphRN",
        "colab_type": "text"
      },
      "source": [
        "This notebook follows [this amazing tutorial on GANs](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f) and tries to port the code to TensorFlow 2.0. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn636Xcba-Q1",
        "colab_type": "text"
      },
      "source": [
        "## Install `Tensorflow 2.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp5esV6aANHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jIXCCVsbC-K",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGB-ehhCAsO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7EUQq5NbFSe",
        "colab_type": "text"
      },
      "source": [
        "## Helper function to generate a distribution (normal) for the real data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G6PMW-RBTpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_distribution_sampler(mu, sigma):\n",
        "  return lambda n: tf.convert_to_tensor(np.random.normal(mu, sigma, (1, n)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loyhvfRxbLX2",
        "colab_type": "text"
      },
      "source": [
        "## Helper function to generate a uniform distribution for the generator network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzGT3Q8zBzht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator_input_sampler():\n",
        "  return lambda m, n: tf.convert_to_tensor(np.random.rand(m, n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG6Py0Xmbczy",
        "colab_type": "text"
      },
      "source": [
        "## The Generator network class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ifARasTCMw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(keras.Model):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.map1 = keras.layers.Dense(hidden_size, input_shape=input_size, activation='tanh')\n",
        "    self.map2 = keras.layers.Dense(hidden_size, activation='tanh')\n",
        "    self.map3 = keras.layers.Dense(output_size, activation='linear')\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.map1(inputs)\n",
        "    x = self.map2(x)\n",
        "    x = self.map3(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZSWuojAbgDE",
        "colab_type": "text"
      },
      "source": [
        "## The Discriminator network class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAAzJHCFEp9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(keras.Model):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.map1 = keras.layers.Dense(hidden_size, input_shape=input_size, activation='sigmoid')\n",
        "    self.map2 = keras.layers.Dense(hidden_size, activation='sigmoid')\n",
        "    self.map3 = keras.layers.Dense(output_size, activation='sigmoid')\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.map1(inputs)\n",
        "    return self.map3(self.map2(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-PY9BZuFgqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_moments(d):\n",
        "  # https://stats.stackexchange.com/questions/126346/why-kurtosis-of-a-normal-distribution-is-3-instead-of-0\n",
        "  # Return the first 4 moments of the data provided\n",
        "  d = tf.transpose(d, (1, 0))\n",
        "  mean = tf.reduce_mean(d)\n",
        "  diffs = (d - mean)\n",
        "  var = tf.reduce_mean(tf.pow(diffs, 2.0))\n",
        "  std = tf.sqrt(var)\n",
        "  zscores = diffs / std\n",
        "  skews = tf.reduce_mean(tf.pow(zscores, 3.0))\n",
        "  kurtoses = tf.reduce_mean(tf.pow(zscores, 4.0)) - 3.0 # excess kurtosis, should be 0 for Gaussian\n",
        "  return tf.stack([mean, std, skews, kurtoses], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OV4KWgRJy7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stats(d):\n",
        "    return [np.mean(d), np.std(d)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzKl011RXlLU",
        "colab_type": "text"
      },
      "source": [
        "## Model hyperparameters and other constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qj45CReOW9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model parameters\n",
        "g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
        "g_hidden_size = 5     # Generator complexity\n",
        "g_output_size = 1     # Size of generated output vector\n",
        "d_input_size = 500    # Minibatch size - cardinality of distributions\n",
        "d_hidden_size = 10    # Discriminator complexity\n",
        "d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
        "minibatch_size = d_input_size\n",
        "\n",
        "d_learning_rate = 1e-3\n",
        "g_learning_rate = 1e-3\n",
        "sgd_momentum = 0.9\n",
        "\n",
        "num_epochs = 5000\n",
        "print_interval = 100\n",
        "d_steps = 20\n",
        "g_steps = 20\n",
        "\n",
        "dfe, dre, ge = 0, 0, 0\n",
        "d_real_data, d_fake_data, g_fake_data = None, None, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdRJkiDAXwQk",
        "colab_type": "text"
      },
      "source": [
        "## Data generation parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM9ZeUMqOt8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_mean = 4\n",
        "data_stddev = 1.25\n",
        "\n",
        "d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
        "gi_sampler = get_generator_input_sampler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIYpY99oXyiQ",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LN_PKnbO4Ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G = Generator(input_size=(500, 1),\n",
        "                  hidden_size=g_hidden_size,\n",
        "                  output_size=g_output_size)\n",
        "\n",
        "D = Discriminator(input_size=(1,4),\n",
        "                  hidden_size=d_hidden_size,\n",
        "                  output_size=d_output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWvzGpJcX2AQ",
        "colab_type": "text"
      },
      "source": [
        "## Declare the loss and optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkc9zVjCPlaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = tf.keras.losses.BinaryCrossentropy(from_logits=True)  \n",
        "d_optimizer = tf.keras.optimizers.SGD(learning_rate=d_learning_rate, momentum=sgd_momentum)\n",
        "g_optimizer = tf.keras.optimizers.SGD(learning_rate=g_learning_rate, momentum=sgd_momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeBmj8g9X8FV",
        "colab_type": "text"
      },
      "source": [
        "## One forward and backward pass with the Discriminator network with real data\n",
        "\n",
        "*We do not update the parameters with these gradients.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kw8MU7Vgr1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# d_real_data = d_sampler(d_input_size)\n",
        "\n",
        "# with tf.GradientTape() as tape:\n",
        "#   d_real_decision = D(get_moments(d_real_data).reshape((1,4)))\n",
        "#   d_real_error = criterion(d_real_decision, np.ones((1,1)))  # ones = true\n",
        "# d_real_grads = tape.gradient(d_real_error, D.trainable_weights) # compute/store gradients, but don't change params\n",
        "# d_real_grads[0].numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81YBUAh6YGc2",
        "colab_type": "text"
      },
      "source": [
        "## One forward and backward pass with the Discriminator network with the fake data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjcWC0gNibld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# d_gen_input = gi_sampler(minibatch_size, g_input_size)\n",
        "# with tf.GradientTape() as tape:\n",
        "#   with tape.stop_recording():\n",
        "#     d_fake_data = G(d_gen_input)\n",
        "#   d_fake_decision = D(get_moments(d_fake_data.numpy().T).reshape((1,4)))\n",
        "#   d_fake_error = criterion(d_fake_decision, np.zeros((1,1)))\n",
        "# d_fake_grads = tape.gradient(d_fake_error, D.trainable_weights) \n",
        "# print(d_fake_grads[0].numpy())\n",
        "# d_optimizer.apply_gradients(zip(d_fake_grads, D.trainable_weights)) # Only optimizes D's parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS7mg238Yf3M",
        "colab_type": "text"
      },
      "source": [
        "## One forward and backward pass with the Generator network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp3eWwowQsfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gen_input = gi_sampler(minibatch_size, g_input_size)\n",
        "# with tf.GradientTape() as tape:\n",
        "#   g_fake_data = G(gen_input)\n",
        "#   dg_fake_decision = D(tf.reshape(get_moments_tf(g_fake_data), (1, 4)))\n",
        "#   g_error = criterion(dg_fake_decision, np.ones((1,1)))\n",
        "# g_grads = tape.gradient(g_error, G.trainable_weights)\n",
        "# g_optimizer.apply_gradients(zip(g_grads, G.trainable_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as0XOQuhQVai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for d_index in range(d_steps):\n",
        "        #  1A: Train D on real\n",
        "        d_real_data = d_sampler(d_input_size)\n",
        "        with tf.GradientTape() as tape:\n",
        "          d_real_decision = D(tf.reshape(get_moments(d_real_data), (1,4)))\n",
        "          d_real_error = criterion(d_real_decision, np.ones((1,1)))  # ones = true\n",
        "        d_real_grads = tape.gradient(d_real_error, D.trainable_weights) # compute/store gradients, but don't change params\n",
        "        \n",
        "        #  1B: Train D on fake\n",
        "        d_gen_input = gi_sampler(minibatch_size, g_input_size)\n",
        "        with tf.GradientTape() as tape:\n",
        "          with tape.stop_recording():\n",
        "            d_fake_data = G(d_gen_input)\n",
        "          d_fake_decision = D(tf.reshape(get_moments(d_fake_data), (1, 4)))\n",
        "          d_fake_error = criterion(d_fake_decision, np.zeros((1,1)))\n",
        "        d_fake_grads = tape.gradient(d_fake_error, D.trainable_weights) \n",
        "        d_optimizer.apply_gradients(zip(d_fake_grads, D.trainable_weights)) # Only optimizes D's parameters\n",
        "\n",
        "        dre, dfe = d_real_error.numpy(), d_fake_error.numpy()\n",
        "\n",
        "    for g_index in range(g_steps):\n",
        "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
        "        gen_input = gi_sampler(minibatch_size, g_input_size)\n",
        "        with tf.GradientTape() as tape:\n",
        "          g_fake_data = G(gen_input)\n",
        "          dg_fake_decision = D(tf.reshape(get_moments(g_fake_data), (1, 4)))\n",
        "          g_error = criterion(dg_fake_decision, np.ones((1,1)))\n",
        "        g_grads = tape.gradient(g_error, G.trainable_weights)\n",
        "        g_optimizer.apply_gradients(zip(g_grads, G.trainable_weights))\n",
        "\n",
        "        ge = g_error.numpy()\n",
        "\n",
        "    if epoch % print_interval == 0:\n",
        "        print(\"Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) \" %\n",
        "              (epoch, dre, dfe, ge, stats(d_real_data), stats(d_fake_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB1GPjYvT8SY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}